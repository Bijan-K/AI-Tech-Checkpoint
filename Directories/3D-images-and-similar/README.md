# 3D Images
- [NeRF](#nerf)
  - [LeRF](#lerf)
  - [MixNeRF](#mixnerf)
  - [F2-NeRF](#f2-nerf)
  - [BungeeNeRF](#bungeenerf)
  - [GridNeRF](#gridnerf)
  - [NSFF](#nsff)
  - [ReLight My NeRF](#relight-my-nerf)
  - [MSNeRF](#msnerf)
  - [DyNeRF](#dynerf)
  - [NeRFPlayer](#nerfplayer)
- [NeRF-SR](#nerf-sr)
- [DreamFusion](#dreamfusion)
- [ProlificDreamer]()
- [Neuralangelo](#neuralangelo)
- [Luma Labs AI](#luma-labs-ai)
- [InstructPix2Pix](#instructpix2pix)
- [3D Video Loops from Asynchronous Input](#3d-video-loops-from-asynchronous-input)

## NeRF
stands for: Representing Scenes as Neural Radiance Fields for View Synthesis. Simply put, uses multiplue images to construct a scene that can be view from multipule angles.
- [Website](https://www.matthewtancik.com/nerf)
- [Github](https://github.com/bmild/nerf)
- [Paper](https://arxiv.org/abs/2003.08934)

## LeRF
- [Paper](https://arxiv.org/abs/2303.09553)
- [Page](https://www.lerf.io)
- [Github](https://github.com/kerrj/lerf)
- [NeRF Studio](https://docs.nerf.studio)

## MixNeRF
- [Paper](https://arxiv.org/abs/2302.08788)
- [Page](https://shawn615.github.io/mixnerf/)
- [Github](https://github.com/shawn615/MixNeRF)


## F2-NeRF
- [Paper](https://arxiv.org/abs/2303.15951)
- [Github](https://github.com/totoro97/f2-nerf)

## BungeeNeRF
- [Paper](https://arxiv.org/abs/2112.05504)
- [Page](https://city-super.github.io/citynerf/)
- [Github](https://github.com/city-super/BungeeNeRF)

## GridNeRF
- [Paper](https://arxiv.org/abs/2303.14001)
- [Page](https://city-super.github.io/gridnerf/)

## NSFF
- [Paper](https://arxiv.org/abs/2303.14001)
- [Page](https://city-super.github.io/gridnerf/)

## ReLight My NeRF
- [Paper](https://arxiv.org/abs/2304.10448)
- [Page](https://eyecan-ai.github.io/rene/)

## MSNeRF
- [Paper](https://arxiv.org/abs/2305.04268)
- [Page](https://zx-yin.github.io/msnerf/)
- [Github](https://github.com/ZX-Yin/ms-nerf)

## DyNeRF
- [Paper](https://arxiv.org/abs/2103.02597)
- [Page](https://neural-3d-video.github.io)

## NeRFPlayer
- [Paper](https://arxiv.org/pdf/2210.15947.pdf)
- [Page](https://lsongx.github.io/projects/nerfplayer.html)

## NeRF-SR
- [Github](https://github.com/cwchenwang/NeRF-SR)

## DreamFusion
Text-to-3D using 2D Diffusion
- [Website](https://dreamfusion3d.github.io)
- [Paper](https://arxiv.org/abs/2209.14988)

## Neuralangelo
- [Paper](https://research.nvidia.com/labs/dir/neuralangelo/paper.pdf)
- [Page](https://research.nvidia.com/labs/dir/neuralangelo/)

## Luma Labs AI
- [Website](https://lumalabs.ai)
- [App](https://apps.apple.com/in/app/luma-ai/id1615849914) 

## InstructPix2Pix
- [Paper](https://arxiv.org/abs/2211.09800)
- [Page](https://www.timothybrooks.com/instruct-pix2pix/)
- [Github](github.com/timothybrooks/instruct-pix2pix)

## 3D Video Loops from Asynchronous Input
- [Paper](https://arxiv.org/abs/2303.05312)
- [Page](https://limacv.github.io/VideoLoop3D_web/)
- [Github](https://github.com/limacv/VideoLoop3D)

---

# Text to 4D

## Text-To-4D Dynamic Scene Generation
Since they were the first to do this, they named it "4D". it's basically text to video but also quite different. You could think of the forth dimension as time, I guess.

- [Website](https://make-a-video3d.github.io)
- [Paper](https://arxiv.org/abs/2301.11280)

[Back to Contents](#contents)
