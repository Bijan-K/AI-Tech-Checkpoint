# Code Generation
Since most popular LLMs fit this role. this can be a bit tough.

> Models

- [Claude 3](https://www.anthropic.com/news/claude-3-family)
- [Chatgpt's GPT-4](https://openai.com/research/gpt-4)
- [Code Llama2](https://about.fb.com/news/2023/08/code-llama-ai-for-coding/)
- [Phind](https://www.phind.com/search?home=true)
- [Mixtal-8x7b-instruct-v0.1](https://huggingface.co/mistralai/Mixtral-8x7B-Instruct-v0.1)
- [Deepseek-coder-33b-instruct](https://huggingface.co/deepseek-ai/deepseek-coder-33b-instruct)

> Editor..ish:

- [Github Copilot](https://github.com/features/copilot)
- [Cursor](https://cursor.sh)
- [Cody](https://sourcegraph.com/cody)
- [Supermaven](https://supermaven.com)
- [Codeium](https://codeium.com)
- [TabNine](https://www.tabnine.com)
- [DevGpt](https://www.devgpt.com)

> ???
- [Devin?](https://x.com/cognition_labs/status/1767548763134964000?s=20)

Welp. I don't think LLMs are suitable for logic but who knows what happens with enough scale. Would be funny to watch companies go brankrupted because they thought LLMs can actually reason tho.

LLMs are token-predictors the best(after a lot of training) they can do is parrot someone else's logic in the form of text.